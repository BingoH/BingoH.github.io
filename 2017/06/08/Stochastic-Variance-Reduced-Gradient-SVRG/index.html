<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="convex optimization," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="This is a non-state-of-art read through of Stochastic Variance Reduced Gradient (SVRG) [1] method. Gradient descent and stochastic gradient descent (SGD) plays the most important role in optimization">
<meta property="og:type" content="article">
<meta property="og:title" content="Stochastic Variance Reduced Gradient (SVRG)">
<meta property="og:url" content="https://bingoh.github.io/2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/index.html">
<meta property="og:site_name" content="Bingo.H's blog">
<meta property="og:description" content="This is a non-state-of-art read through of Stochastic Variance Reduced Gradient (SVRG) [1] method. Gradient descent and stochastic gradient descent (SGD) plays the most important role in optimization">
<meta property="og:image" content="https://bingoh.github.io/images/convxopt/svrg.png">
<meta property="og:updated_time" content="2017-06-13T19:43:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stochastic Variance Reduced Gradient (SVRG)">
<meta name="twitter:description" content="This is a non-state-of-art read through of Stochastic Variance Reduced Gradient (SVRG) [1] method. Gradient descent and stochastic gradient descent (SGD) plays the most important role in optimization">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://bingoh.github.io/2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/"/>





  <title>Stochastic Variance Reduced Gradient (SVRG) | Bingo.H's blog</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  















  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bingo.H's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Don't panic!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://bingoh.github.io/2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Bingo.H">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bingo.H's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Stochastic Variance Reduced Gradient (SVRG)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-08T16:35:16-04:00">
                2017-06-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Optimization/" itemprop="url" rel="index">
                    <span itemprop="name">Optimization</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is a non-state-of-art read through of Stochastic Variance Reduced Gradient (SVRG) <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> method. Gradient descent and stochastic gradient descent (SGD) plays the most important role in optimization of machine learning problems. With large scale datasets, especially in deep learning applications, SGD and its variants maybe the only practical choice. This paper proposes to accelerate the convergence of SGD by reducing the variance of gradient, introduced by random sampling when evaluating gradient. This work has been extended to many other problems, such as non-convex optimization, sparse learning, etc. So this post is just a late bird note.</p>
<a id="more"></a>
<h2>Gradient Descent and Stochastic Gradient Descent</h2>
<p>In machine learning, we usually consider the following optimization problem, $$ \min P(w), \quad P(w) := \frac{1}{n} \sum_{i=1}^n \psi_i(w), $$ where $w$ represents the model parameter and $\psi_i(w)$ is a sequence of loss functions that evaluate the cost of current parameter $w$. Usually, $\psi_i(w)$ depends on training data $(x_i, y_i)$ (supervised learning).</p>
<p><strong>Examples</strong></p>
<ul>
<li>square loss: $\psi(w) = (w^T x_i - y_i)^2$</li>
<li>log loss: $-y_i \ln(\sigma(w^T x_i)) - (1-y_i) \ln(1- \sigma(w^T x_i)) = \ln(1 + \exp(-y_i w^T x_i))$, $\sigma(\cdot)$ is the sigmoid function</li>
</ul>
<p><strong>Gradient Descent</strong><br>
Gradient descent is derived from the taylor expansion of a function. For smooth $P(w)$, in a small neighborhood of $w_0$, we have $$ P(w) \approx P(w_0) + \langle x - x_0, \nabla f(x_0) \rangle $$</p>
<p>Or along an arbitrary unit direction $d$, we have for small $t &gt; 0$, $$ P(w_0 + td) \approx P(w_0) + t \nabla P(w_0)^T d $$<br>
It turns out that for the negative $d = -\nabla P(w_0)$ is a locally descent direction, that is, for small $t&gt;0$, the following inequality holds, $$ P(w_0 - t \nabla P(w_0)) \approx P(w_0) - t \| \nabla P(w_0)\|_2^2 \leq P(w_0) $$</p>
<p>This leads the standard gradient descent method, $$ w_t = w_{t-1} - \eta_t \nabla P(w_{t-1}) $$<br>
In our problem, we have $\nabla P(w_{t-1}) = \frac{1}{n} \sum_{i=1}^n \nabla \psi_i(w_{t-1})$. I need another post to review the selection of step length $\eta_t$ (aka. learning rate in machine learning), the convergence of gradient descent and recent theoretical results.</p>
<p><strong>Stochastic Gradient Descent</strong><br>
At each step, gradient descent requires evaluations of $n$ derivatives. It is very expensive for large scale problems, which is common in machine leanring. A popular variant is SGD method with the following update rule, $$ w_t = w_{t-1} - \eta_t \nabla \psi_{i_t}(w_{t-1}), $$<br>
where $i_t$ is randomly drawn from  $\{1, 2, \ldots, n \}$ at each iteration $t$. We have $\mathbb{E}[\nabla \psi_{i_t}(w_{t-1}) | w_{t-1}] = \nabla P(w_{t-1})$.</p>
<p><strong>Generalized form</strong><br>
The SGD updating rule can be further generalized, $$ w_t = w_{t-1} - \eta_t g_t(w_{t-1}, \xi_t) $$<br>
where $\xi_t$ is a random variable that may depend on $w_{t-1}$ and $g_t(\cdot)$ is an approximate gradient. We only require it is an unbiased estimator, $\nabla P(w_{t-1}) = \mathbb{E}_{\xi_t}[g_t(w_{t-1}, \xi_t)|w_{t-1}]$. For example, the widely used mini-batch version, $$ g_t(w_{t-1}, \xi_t) = \frac{1}{m} \sum_{i=1}^m \nabla \psi_{i_m}(w_{t-1}). $$</p>
<p>Actually, many algorithms use a biased estimator, especially in neural network training. I need another post to reivew the SGD variants and the theoretical results.</p>
<h2>Why SVRG</h2>
<p><strong>Disadvantages of GD and SGD</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Time Complexity</th>
<th style="text-align:center">learning rate</th>
<th style="text-align:center">Convergence Rate (Stronly Convex)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">GD</td>
<td style="text-align:center">n gradient evaluataion</td>
<td style="text-align:center">const.</td>
<td style="text-align:center">O(log t)</td>
</tr>
<tr>
<td style="text-align:center">SGD</td>
<td style="text-align:center">1 gradient evaluation</td>
<td style="text-align:center">O(1/t)</td>
<td style="text-align:center">O(1/t)</td>
</tr>
</tbody>
</table>
<p>As the stochastic gradient is just a random approximation (by a small batch of samples or even a single example) of the batch gradient, we must be careful when updating along the direction. In order to ensure convergence, the learning rate $\eta_t$ has to decay to zero, due to the variance of random sampling. We generally choose $\eta_t = O(1/t)$. Small learning rate leads to solwer sub-linear convergence rate of $O(1/t)$. We have a trade-off between the computation per iteration and convergence rate.</p>
<p>Fortunately, one can design methods that can reduce the variance of stochastic gradient. This may allow us to use a larger learning rate for SGD.</p>
<h2>SVRG algorithm</h2>
<p>The proposed SVRG algorithm updates by the following rule, $$<br>
w_t = w_{t-1} - \eta_t (\nabla \psi_{i_t}(w_{t-1}) - \nabla \psi_{i_t}(\tilde{w}) + \nabla P(\tilde{w})), $$ where $\tilde{w}$ is a snapshot that is updated every $m$ SGD iterations.</p>
<p><strong>Intuitions</strong><br>
If the snapshot $\tilde{w}$ is close to optima $w^\ast$, $\nabla \psi_i(\tilde{w}) \rightarrow \nabla \psi_i(w^\ast)$,</p>
<ul>
<li>Let $\tilde{\mu} := \nabla P(\tilde{w})$, then $\tilde{\mu} - \nabla P(w_{t-1}) \approx \nabla \psi_i(\tilde{w}) - \nabla \psi_i(w_{t-1})$. Intuitively, this updating rule cancel the randomness induced by random sampling $i$;</li>
<li>$\tilde{\mu} \rightarrow 0$ when $\tilde{w} \rightarrow w^\ast$. $\nabla \psi_i(w_{t-1}) - \nabla \psi_i(\tilde{w}) + \tilde{\mu} \rightarrow \nabla \psi_i(w_{t-1}) - \nabla \psi_i(w^\ast) \rightarrow 0$. The infinite small gradient allows to use constant learning rate. However, for SGD, $\nabla \psi_i(w_{t-1})$ may not converge to 0.</li>
</ul>
<p><strong>Algorithm</strong><br>
<img src="/images/convxopt/svrg.png" alt="SVRG"></p>
<p><strong>Theorem</strong></p>
<blockquote>
<p>Suppose $\gamma$-smooth $\psi_i$ and $L$-strongly convex $P(w)$. Let $w^\ast = \arg \min_w P(w)$. We have geometric convergence in expectation for SVRG: $$\mathbb{E} P(\tilde{w}_s) - \mathbb{E} P(w^\ast) \leq \alpha^s (P(\tilde{w}_0) - P(w^\ast))$$ given $m$ is sufficiently large, s.t., $$\alpha = \frac{1}{\gamma \eta m(1-2L\eta)} + \frac{2L \eta}{1-2L \eta} &lt; 1$$</p>
</blockquote>
<p>Actually, $m$ is of the same order of $n$ in the paper. Thus it might be more precise to say the following convergence rate, $$ \mathbb{E} P(\tilde{w}_s) - \mathbb{E} P(w^\ast) \leq \alpha^{t/n} (P(\tilde{w}_0) - P(w^\ast))$$</p>
<h2>Summary</h2>
<ul>
<li>Randomness of SGD induces variance of gradient, which leads to decay learning rate and sub-linear convergence rate</li>
<li>Reducing the variance of stochastic gradient allows to use constant learning rate and obtains linear convergence in expectation</li>
<li>We donnot need to save the historical gradient $\nabla \psi_{i_1}(w_0), \nabla \psi_{i_2}(w_1), \ldots$ in SVRG. However, the number of gradient evaluation per iteration increases</li>
<li>SVRG may be applied to non-strongly convex problem, leading a $O(1/T)$ convergence rate (standard SGD $O(1/\sqrt{T})$)</li>
<li>SVRG can also be used to non-convex optimization problem, such as neural networks training</li>
</ul>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Johnson, Rie, and Tong Zhang. “Accelerating stochastic gradient descent using predictive variance reduction.” In Advances in Neural Information Processing Systems, pp. 315-323. 2013. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/convex-optimization/" rel="tag"># convex optimization</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/03/17/Notes-on-Convex-Optimization/" rel="next" title="Notes on Convex Optimization">
                <i class="fa fa-chevron-left"></i> Notes on Convex Optimization
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/13/Notes-on-Convex-Optimization-3-Optimal-Method-for-Smooth-Convex-Optimization/" rel="prev" title="Notes on Convex Optimization (3): Optimal Method for Smooth Convex Optimization">
                Notes on Convex Optimization (3): Optimal Method for Smooth Convex Optimization <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Bingo.H" />
          <p class="site-author-name" itemprop="name">Bingo.H</p>
           
              <p class="site-description motion-element" itemprop="description">Bingo.H's blog</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">Gradient Descent and Stochastic Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">Why SVRG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">SVRG algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bingo.H</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://BingoH.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://bingoh.github.io/2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/';
          this.page.identifier = '2017/06/08/Stochastic-Variance-Reduced-Gradient-SVRG/';
          this.page.title = 'Stochastic Variance Reduced Gradient (SVRG)';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://BingoH.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  













  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
